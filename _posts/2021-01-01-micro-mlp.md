---
layout: post
title: "[DRAFT] A Multilayer Perceptron in C++"
date:   2021-01-01 17:52:54
---

**TL;DR**: This post presents a simple multilayer perceptron class in C++.

---

## Introduction 

This post serves as an introduction to the working horse algorithm in deep learning, the backpropagation stochastic gradient descent algorithm, and shows how this algorithm can be implemented in C++. Throughout the post, a multilayer perceptron network with three hidden layers serves as an example. 

![](/assets/images/post7/mlp.png)

For better understanding, the implementation is explicit, but can be generalized later without much effort. A simple classification task serves as an example.

## Related Work

...

## Method

The implementation of a fully connected neural network can be divided into two parts. The feedforward part, that consists of the mapping from input to output space, and the backpropagation gradient descent part, where the network actually learns.

In a multilayer perceptron, the feedforward process, i.e. the mapping of information, consists of simple affine transformations followd by a nonlinear transformation of the form

\begin{equation} \label{eq:affinetransformation}
    z = \sum_i w_i x_i
\end{equation}

\begin{equation} \label{eq:nonlineartransformation}
    x = h(z) 
\end{equation}

For the implementation of a network, we need to derive the optimization algorithm.

## Implementation

...

## Experiments

...

## Results

...

## Discussion

...

## Outlook

...

---

You find the code for this project [here][github_code].

<!-- Links -->

[github_code]: https://github.com/KaiFabi/MicroMLP
