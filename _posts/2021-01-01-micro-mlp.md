---
layout: post
title: "[DRAFT] A Multilayer Perceptron in C++"
date:   2021-01-01 17:52:54
---

**TL;DR**: This post presents a simple multilayer perceptron class in C++.

---
## TODO:

- bla

## Introduction 

This post serves as an introduction to the working horse algorithm in deep learning, the backpropagation stochastic gradient descent algorithm, and shows how this algorithm can be implemented in C++. Throughout the post, a multilayer perceptron network with three hidden layers serves as an example. 

For better understanding, the implementation is explicit, but can be generalized later without much effort. A simple classification task serves as an example.

## Related Work

...

## Method

The implementation of a fully connected neural network can be divided into two parts. The feedforward part, that consists of the mapping from input to output space, and the backpropagation gradient descent part, where the network actually learns.

In a multilayer perceptron, the feedforward process, i.e. the mapping of information from the input to the output dimension, consists of affine transformations followd by a nonlinear transformation.

\begin{equation} \label{eq:affinetransformation}
    z = \sum_i w_i x_i
\end{equation}

\begin{equation} \label{eq:nonlineartransformation}
    x = h(z) 
\end{equation}

The following visualization of our three layered fully connected neural network shall serve for a better understanding of the derivations in this section.

{:refdef: style="text-align: center;"}
![](/assets/images/post7/mlp3.png)
{: refdef}

For our three layer neural network, the mapping can be formulated as follows, starting at the input of the network:

Input $\rightarrow$ Hidden 1

\begin{equation} \label{eq:z1}
    z_l = \sum_m w_{lm} x_m + b_l
\end{equation}

\begin{equation} \label{eq:x1}
    x_l = h(z_l) 
\end{equation}

Hidden 1 $\rightarrow$ Hidden 2

\begin{equation} \label{eq:z2}
    z_k = \sum_l w_{kl} x_l + b_k
\end{equation}

\begin{equation} \label{eq:x2}
    x_k = h(z_k) 
\end{equation}

Hidden 2 $\rightarrow$ Hidden 3

\begin{equation} \label{eq:z3}
    z_j = \sum_k w_{jk} x_k + b_j
\end{equation}

\begin{equation} \label{eq:x3}
    x_j = h(z_j) 
\end{equation}

Hidden 3 $\rightarrow$ Output

\begin{equation} \label{eq:z4}
    z_i = \sum_j w_{ij} x_j + b_i
\end{equation}

\begin{equation} \label{eq:x4}
    x_i = h(z_i) 
\end{equation}

Here, $h(\cdot)$ represents an activation function, that transforms its input in a nonlinear fashion, and $x_i$ represents the network's neuron-wise prediction at the output layer.

After successfully mapping the input signal to the network's output, the error of the prediction can now be determined. For this purpose, one can make use of the simple L2 loss function, which is defined as follows

\begin{equation} \label{eq:loss_function}
    L = \frac{1}{2} \sum_i (y_i - x_i)^2
\end{equation}

where $y_i$ represents the true label that the network is trying to predict.

Now that we have described the feedforward process as well as the computation of the loss, we can now use the information gathered during the feedforward process, such as activations and preactivations, to compute the gradients for every trainable parameter in our network. This is also the first part to derive an optimization algorithm that allows us to train our network.

In contrast to the feedforward process, now we start at the network's output and work our way back to the input layer. Since the loss function is a multiple nested function, we can use the chain rule of calculus to compute the desired gradients.

We start by looking at how the weights connecting the last two layers affect the loss function.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients4}
    \frac{dL}{dw_{ij}} &=& \frac{dz_i}{dw_{ij}} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                       &=& x_j \cdot \underbrace{h'(z_i) \cdot (x_i - y_i)}_{\delta_i}
\end{gathered}
\end{equation}
$$

The gradients for the biases are determined analogously. 

$$
\begin{equation}
\begin{gathered} \label{eq:gradients4b}
    \frac{dL}{db_{i}} &=& \frac{dz_i}{db_{i}} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                       &=& 1 \cdot \underbrace{h'(z_i) \cdot (x_i - y_i)}_{\delta_i}
\end{gathered}
\end{equation}
$$

As can be seen, the expressions are almost identical except for the first term. For this reason, the explicit derivation of the gradients for the biases will be omitted in the following.

Here, I identified the last two term in Equation \eqref{eq:gradients4} with $\delta_i$. Later on, this allows to compute the gradients in a more efficient way. This is the first rule that tells us how we can compute the gradients for all $w_{ij}$.

Now that we have determined the first gradients, we can proceed to the calculation of the gradients for the weights connecting the last two hidden layers. Since from now on each weight influences the activity of numerous neurons in deeper layers, in contrast to just now, it must taken into account that the error signals for each individual weight and bias must be summed up.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients3}
    \frac{dL}{dw_{jk}} &=& \frac{dz_j}{dw_{jk}} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                       &=& x_k \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:gradients3b}
    \frac{dL}{db_{j}} &=& \frac{dz_j}{db_{j}} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                      &=& 1 \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}
\end{gathered}
\end{equation}
$$

Equation \eqref{eq:gradients3} shows that certain terms, marked by a $\delta$ occur repeatedly durig the gradient calculation. This can be exploited in the implementation in order not to have to calculate certain quantities over and over again. 

Let's now have a look at the weights connecting the first two hidden layers. We can compute the gradients in the same fashion and we'll see that a repeating patter occurs.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients2}
\frac{dL}{dw_{kl}} 
&=& \frac{dz_k}{dw_{kl}} \cdot \frac{dx_k}{dz_k} \cdot \sum_j \frac{dz_j}{dx_k} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
&=& x_l \cdot h'(z_k) \cdot \sum_j w_{jk} \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}\\
&=& x_l \cdot \underbrace{h'(z_k) \cdot \sum_j w_{jk} \delta_j}_{\delta_k}
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:gradients2b}
\frac{dL}{db_{k}} 
&=& \frac{dz_k}{db_{k}} \cdot \frac{dx_k}{dz_k} \cdot \sum_j \frac{dz_j}{dx_k} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
&=& 1 \cdot h'(z_k) \cdot \sum_j w_{jk} \delta_j
\end{gathered}
\end{equation}
$$

Finally, we can now compute the gradients for the weights connecing the input to the first hidden layer.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients1}
\frac{dL}{dw_{lm}} 
&=& \frac{dz_l}{dw_{lm}} \cdot \frac{dx_l}{dz_l} \cdot \sum_k \frac{dz_k}{dx_l} \cdot \frac{dx_k}{dz_k} \cdot \sum_j \frac{dz_j}{dx_k} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
&=& x_m \cdot \underbrace{h'(z_l) \cdot \sum_k w_{kl} \cdot h'(z_k) \cdot \sum_j w_{jk} \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}}_{\delta_l}\\
&=& x_m \cdot \underbrace{h'(z_l) \cdot \sum_k w_{kl} \delta_k}_{\delta_l}
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:gradients1b}
\frac{dL}{db_{l}} 
&=& 1 \cdot h'(z_l) \cdot \sum_k w_{kl} \delta_k
\end{gathered}
\end{equation}
$$


In Equation \eqref{eq:gradients1} it is interesting to see, how the error signal from the network's output is carried all the way back to the weights connecting the frist two layers of our network. 

The only thing missing now is the gradient descent by updating the weights and biases using the gradients that we have just computed:

$$
\begin{equation}
\begin{gathered} \label{eq:update4}
w_{ij}^{(t+1)} = w_{ij}^{(t)} - \eta \cdot \frac{dL}{dw_{ij}^{(t+1)}} , \qquad b_{i}^{(t+1)} = b_{i}^{(t)} - \eta \cdot \frac{dL}{db_{i}^{(t+1)}} 
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:update3}
w_{jk}^{(t+1)} = w_{jk}^{(t)} - \eta \cdot \frac{dL}{dw_{jk}^{(t+1)}} , \qquad b_{j}^{(t+1)} = b_{j}^{(t)} - \eta \cdot \frac{dL}{db_{j}^{(t+1)}} 
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:update2}
w_{kl}^{(t+1)} = w_{kl}^{(t)} - \eta \cdot \frac{dL}{dw_{kl}^{(t+1)}} , \qquad b_{k}^{(t+1)} = b_{k}^{(t)} - \eta \cdot \frac{dL}{db_{k}^{(t+1)}} 
\end{gathered}
\end{equation}
$$


$$
\begin{equation}
\begin{gathered} \label{eq:update1}
w_{lm}^{(t+1)} = w_{lm}^{(t)} - \eta \cdot \frac{dL}{dw_{lm}^{(t+1)}} , \qquad b_{l}^{(t+1)} = b_{l}^{(t)} - \eta \cdot \frac{dL}{db_{l}^{(t+1)}} 
\end{gathered}
\end{equation}
$$

Here I have marked the current training iteration by $()^{(t+1)}$. The learning rate is given by $\eta$.

Now that we are able to compute the necessary gradients to train our network, we can move on to the next section where we're going to implement the equations derived above.

## Implementation

```c++
void NeuralNetwork::feedforward() {
    z1 = matmul(W1, x0);
    x1 = relu(z1);
    z2 = matmul(W2, x1);
    x2 = relu(z2);
    z3 = matmul(W3, x2);
    x3 = relu(z3);
    z4 = matmul(W4, x3);
    x4 = sigmoid(z4);
}
```

```c++
std::vector<double> NeuralNetwork::relu_prime(const std::vector<double>& z) {
    std::vector<double> d_relu(z.size());
    for (unsigned int i=0; i<z.size(); ++i) {
        if (z[i] >= 0.0) {
            d_relu[i] = 1.0;
        } else {
            d_relu[i] = 0.0;
        }
    }
    return d_relu; 
}
```

## Experiments

...

## Results

...

## Discussion

...

## Outlook

...

---

You find the code for this project [here][github_code].

<!-- Links -->

[github_code]: https://github.com/KaiFabi/MicroMLP
