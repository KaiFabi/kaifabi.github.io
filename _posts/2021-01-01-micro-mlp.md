---
layout: post
title: "[DRAFT] A Multilayer Perceptron in C++"
date:   2021-01-01 17:52:54
---

**TL;DR**: This post presents the derivation and implementation of the stochastic gradient descent algorithm applied to a three-layered multilayer perceptron in C++.

---

## Introduction 

This post serves as an introduction to the working horse algorithm in deep learning, the backpropagation stochastic gradient descent algorithm, and shows how this algorithm can be implemented in C++. Throughout the post, a multilayer perceptron network with three hidden layers serves as an example. 

For better understanding, the implementation is explicit, but can be generalized later without much effort. A simple classification task serves as an example.

## Method

The implementation of a fully connected neural network can be divided into two parts. The feedforward part, that consists of the mapping from input to output space, and the backpropagation gradient descent part, where the network actually learns.

In a multilayer perceptron, the feedforward process, i.e. the mapping of information from the input to the output dimension, consists of affine transformations followd by a nonlinear transformation.

\begin{equation} \label{eq:affinetransformation}
    z = \sum_i w_i x_i
\end{equation}

\begin{equation} \label{eq:nonlineartransformation}
    x = h(z) 
\end{equation}

The following visualization of our three layered fully connected neural network shall serve for a better understanding of the derivations in this section.

{:refdef: style="text-align: center;"}
![](/assets/images/post7/mlp.png)
{: refdef}

For our three layer neural network, the mapping can be formulated as follows, starting at the input of the network:

Input $\rightarrow$ Hidden 1

\begin{equation} \label{eq:z1}
    z_l = \sum_m w_{lm} x_m + b_l
\end{equation}

\begin{equation} \label{eq:x1}
    x_l = h(z_l) 
\end{equation}

Hidden 1 $\rightarrow$ Hidden 2

\begin{equation} \label{eq:z2}
    z_k = \sum_l w_{kl} x_l + b_k
\end{equation}

\begin{equation} \label{eq:x2}
    x_k = h(z_k) 
\end{equation}

Hidden 2 $\rightarrow$ Hidden 3

\begin{equation} \label{eq:z3}
    z_j = \sum_k w_{jk} x_k + b_j
\end{equation}

\begin{equation} \label{eq:x3}
    x_j = h(z_j) 
\end{equation}

Hidden 3 $\rightarrow$ Output

\begin{equation} \label{eq:z4}
    z_i = \sum_j w_{ij} x_j + b_i
\end{equation}

\begin{equation} \label{eq:x4}
    x_i = h(z_i) 
\end{equation}

Here, $h(\cdot)$ represents an activation function, that transforms its input in a nonlinear fashion, and $x_i$ represents the network's neuron-wise prediction at the output layer.

After successfully mapping the input signal to the network's output, the error of the prediction can now be determined. For this purpose, one can make use of the simple L2 loss function, which is defined as follows

\begin{equation} \label{eq:loss_function}
    L = \frac{1}{2} \sum_i (y_i - x_i)^2
\end{equation}

where $y_i$ represents the true label that the network is trying to predict.

Now that we have described the feedforward process as well as the computation of the loss, we can now use the information gathered during the feedforward process, such as activations and preactivations, to compute the gradients for every trainable parameter in our network. This is also the first part to derive an optimization algorithm that allows us to train our network.

In contrast to the feedforward process, now we start at the network's output and work our way back to the input layer. Since the loss function is a multiple nested function, we can use the chain rule of calculus to compute the desired gradients.

We start by looking at how the weights connecting the last two layers affect the loss function.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients4}
    \frac{dL}{dw_{ij}} &=& \frac{dz_i}{dw_{ij}} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                       &=& x_j \cdot \underbrace{h'(z_i) \cdot (x_i - y_i)}_{\delta_i}
\end{gathered}
\end{equation}
$$

The gradients for the biases are determined analogously. 

$$
\begin{equation}
\begin{gathered} \label{eq:gradients4b}
    \frac{dL}{db_{i}} &=& \frac{dz_i}{db_{i}} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                       &=& 1 \cdot \underbrace{h'(z_i) \cdot (x_i - y_i)}_{\delta_i}
\end{gathered}
\end{equation}
$$

As can be seen, the expressions are almost identical except for the first term. For this reason, the explicit derivation of the gradients for the biases will be omitted in the following.

Here, I identified the last two term in Equation \eqref{eq:gradients4} with $\delta_i$. Later on, this allows to compute the gradients in a more efficient way. This is the first rule that tells us how we can compute the gradients for all $w_{ij}$.

Now that we have determined the first gradients, we can proceed to the calculation of the gradients for the weights connecting the last two hidden layers. Since from now on each weight influences the activity of numerous neurons in deeper layers, in contrast to just now, it must taken into account that the error signals for each individual weight and bias must be summed up.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients3}
    \frac{dL}{dw_{jk}} &=& \frac{dz_j}{dw_{jk}} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                       &=& x_k \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:gradients3b}
    \frac{dL}{db_{j}} &=& \frac{dz_j}{db_{j}} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
                      &=& 1 \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}
\end{gathered}
\end{equation}
$$

Equation \eqref{eq:gradients3} shows that certain terms, marked by a $\delta$ occur repeatedly durig the gradient calculation. This can be exploited in the implementation in order not to have to calculate certain quantities over and over again. 

Let's now have a look at the weights connecting the first two hidden layers. We can compute the gradients in the same fashion and we'll see that a repeating patter occurs.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients2}
\frac{dL}{dw_{kl}} 
&=& \frac{dz_k}{dw_{kl}} \cdot \frac{dx_k}{dz_k} \cdot \sum_j \frac{dz_j}{dx_k} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
&=& x_l \cdot h'(z_k) \cdot \sum_j w_{jk} \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}\\
&=& x_l \cdot \underbrace{h'(z_k) \cdot \sum_j w_{jk} \delta_j}_{\delta_k}
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:gradients2b}
\frac{dL}{db_{k}} 
&=& \frac{dz_k}{db_{k}} \cdot \frac{dx_k}{dz_k} \cdot \sum_j \frac{dz_j}{dx_k} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
&=& 1 \cdot h'(z_k) \cdot \sum_j w_{jk} \delta_j
\end{gathered}
\end{equation}
$$

Finally, we can now compute the gradients for the weights connecing the input to the first hidden layer.

$$
\begin{equation}
\begin{gathered} \label{eq:gradients1}
\frac{dL}{dw_{lm}} 
&=& \frac{dz_l}{dw_{lm}} \cdot \frac{dx_l}{dz_l} \cdot \sum_k \frac{dz_k}{dx_l} \cdot \frac{dx_k}{dz_k} \cdot \sum_j \frac{dz_j}{dx_k} \cdot \frac{dx_j}{dz_j} \cdot \sum_i \frac{dz_i}{dx_j} \cdot \frac{dx_i}{dz_i} \cdot \frac{dL}{dx_i}\\
&=& x_m \cdot \underbrace{h'(z_l) \cdot \sum_k w_{kl} \cdot h'(z_k) \cdot \sum_j w_{jk} \cdot \underbrace{h'(z_j) \cdot \sum_i  w_{ij} \cdot \overbrace{h'(z_i) \cdot (x_i - y_i)}^{\delta_i}}_{\delta_j}}_{\delta_l}\\
&=& x_m \cdot \underbrace{h'(z_l) \cdot \sum_k w_{kl} \delta_k}_{\delta_l}
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:gradients1b}
\frac{dL}{db_{l}} 
&=& 1 \cdot h'(z_l) \cdot \sum_k w_{kl} \delta_k
\end{gathered}
\end{equation}
$$


In Equation \eqref{eq:gradients1} it is interesting to see, how the error signal from the network's output is carried all the way back to the weights connecting the frist two layers of our network. 

The only thing missing now is the gradient descent by updating the weights and biases using the gradients that we have just computed:

$$
\begin{equation}
\begin{gathered} \label{eq:update4}
w_{ij}^{(t+1)} = w_{ij}^{(t)} - \eta \cdot \frac{dL}{dw_{ij}^{(t+1)}} , \qquad b_{i}^{(t+1)} = b_{i}^{(t)} - \eta \cdot \frac{dL}{db_{i}^{(t+1)}} 
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:update3}
w_{jk}^{(t+1)} = w_{jk}^{(t)} - \eta \cdot \frac{dL}{dw_{jk}^{(t+1)}} , \qquad b_{j}^{(t+1)} = b_{j}^{(t)} - \eta \cdot \frac{dL}{db_{j}^{(t+1)}} 
\end{gathered}
\end{equation}
$$

$$
\begin{equation}
\begin{gathered} \label{eq:update2}
w_{kl}^{(t+1)} = w_{kl}^{(t)} - \eta \cdot \frac{dL}{dw_{kl}^{(t+1)}} , \qquad b_{k}^{(t+1)} = b_{k}^{(t)} - \eta \cdot \frac{dL}{db_{k}^{(t+1)}} 
\end{gathered}
\end{equation}
$$


$$
\begin{equation}
\begin{gathered} \label{eq:update1}
w_{lm}^{(t+1)} = w_{lm}^{(t)} - \eta \cdot \frac{dL}{dw_{lm}^{(t+1)}} , \qquad b_{l}^{(t+1)} = b_{l}^{(t)} - \eta \cdot \frac{dL}{db_{l}^{(t+1)}} 
\end{gathered}
\end{equation}
$$

Here I have marked the current training iteration by $()^{(t+1)}$. The learning rate is given by $\eta$.

Now that we are able to compute the necessary gradients to train our network, we can move on to the next section where we're going to implement the equations derived above.

## Implementation

In this section, the implementation in C++ will be roughly outlined. I will not go into the generation of the datasets, but only into the core algorithm that is used to train the network. You can find all the code on Github.

### Weight Initialization

Using a good weight initialization scheme is crucial as it often helps the network to converge significantly faster and also allows for much deeper network architectures. To initialize the network's weights connecting the neurons of adjacent layers, the He initialization turned out to be very useful. See also [here][he_paper]. The network's biases are initialized with zero.

```c++
void NeuralNetwork::he_initialization(std::vector<std::vector<double>>& W) {
    double mean = 0.0;
    double std = std::sqrt(2.0 / static_cast<double>(W.size()));
    std::normal_distribution<double> rand_normal(mean, std);
    for (unsigned int i=0; i<W.size(); ++i) {
        for (unsigned int j=0; j<W[0].size(); ++j) {
            W[i][j] = rand_normal(gen);
        }
    } 
}
```

### Activation Functions

The network uses two different types of activation functions. The rectified linear unit (ReLU) in the hidden layers of the network, and the sigmoid function at the network output. The ReLU function for the feedforward pass and its derivative for the backward pass can be implemented in C++ as follows

```c++
std::vector<double> NeuralNetwork::relu(const std::vector<double>& z) {
    std::vector<double> x(z.size());
    for (unsigned int i=0; i<z.size(); ++i) {
        if (z[i] > 0.0) {
            x[i] = z[i];
        } else {
            x[i] = 0.0;
        }
    }
    return x;
}
```

```c++
double NeuralNetwork::relu_prime(const double z) {
    double x;
    if (z >= 0.0) {
        x = 1.0;
    } else {
        x = 0.0;
    }
    return x; 
}
```

At the network's output layer, we use the sigmoid function to squeeze the preactivations into the range between 0 and 1. In order to avoid numerical overflow while using the sigmoid function we can implement a numerically stable version of it. We do this by separating large negative or large positive preactivations.

```c++
std::vector<double> NeuralNetwork::sigmoid(const std::vector<double>& z) {
    std::vector<double> x(z.size());
    for (unsigned int i=0; i<z.size(); ++i) {
        if (z[i] >= 0.0) {
            x[i] = 1.0 / (1.0 + std::exp(-z[i]));
        } else {
            x[i] = std::exp(z[i]) / (1.0 + std::exp(z[i]));
        }
    }
    return x;
}
```

```c++
double NeuralNetwork::sigmoid_prime(const double z) {
    double sigma;
    if (z > 0.0) {
        sigma = 1.0 / (1.0 + std::exp(-z));
    } else {
        sigma = std::exp(z) / (1.0 + std::exp(z));
    }
    return sigma * (1.0 - sigma);
}
```

### Feedforward

The `feedforward()` method uses the activation functions shown above and and the `matmul()` operation that also processes the bias.

```c++
void NeuralNetwork::feedforward() {
    z1 = matmul(W1, x, b1);
    x1 = relu(z1);
    z2 = matmul(W2, x1, b2);
    x2 = relu(z2);
    z3 = matmul(W3, x2, b3);
    x3 = relu(z3);
    z4 = matmul(W4, x3, b4);
    x4 = sigmoid(z4);
}
```

The `matmul()` operation performs an affine transformations of its input `x` using the weights `W` and biases `b` of the corresponding layer and returns the preactivations.

```c++
std::vector<double> NeuralNetwork::matmul(const std::vector<std::vector<double>>& W, 
                                          const std::vector<double>& x, 
                                          const std::vector<double>& b) {
    std::vector<double> z(W.size(), 0.0);
    for (unsigned int i=0; i<W.size(); ++i) {
        for (unsigned int j=0; j<W[0].size(); ++j) {
           z[i] += W[i][j] * x[j];
        }
        z[i] += b[i];
    }
    return z;
}
```


### Backpropagation

A verbose implementation of the backpropagation algorithm.

```c++
void NeuralNetwork::backpropagation() {
    for (unsigned int i=0; i<W4.size(); ++i) {
        delta4[i] = sigmoid_prime(z4[i]) * (x4[i] - y[i]);
    }

    for (unsigned int i=0; i<W4.size(); ++i) {
        for (unsigned int j=0; j<W4[0].size(); ++j) {
            dW4[i][j] = x3[j] * delta4[i];
        }
        db4[i] = delta4[i];
    }

    for (unsigned int j=0; j<W4[0].size(); ++j) {
        double tmp = 0.0;
        for (unsigned int i=0; i<W4.size(); ++i) {
            tmp += W4[i][j] * delta4[i];
        }
        delta3[j] = relu_prime(z3[j]) * tmp;
    }

    for (unsigned int i=0; i<W3.size(); ++i) {
        for (unsigned int j=0; j<W3[0].size(); ++j) {
            dW3[i][j] = x2[j] * delta3[i];
        }
        db3[i] = delta3[i];
    }

    for (unsigned int j=0; j<W3[0].size(); ++j) {
        double tmp = 0.0;
        for (unsigned int i=0; i<W3.size(); ++i) {
            tmp += W3[i][j] * delta3[i];
        }
        delta2[j] = relu_prime(z2[j]) * tmp;
    }

    for (unsigned int i=0; i<W2.size(); ++i) {
        for (unsigned int j=0; j<W2[0].size(); ++j) {
            dW2[i][j] = x1[j] * delta2[i];
        }
        db2[i] = delta2[i];
    }

    for (unsigned int j=0; j<W2[0].size(); ++j) {
        double tmp = 0.0;
        for (unsigned int i=0; i<W2.size(); ++i) {
            tmp += W2[i][j] * delta2[i];
        }
        delta1[j] = relu_prime(z1[j]) * tmp;
    }

    for (unsigned int i=0; i<W1.size(); ++i) {
        for (unsigned int j=0; j<W1[0].size(); ++j) {
            dW1[i][j] = x[j] * delta1[i];
        }
        db1[i] = delta1[i];
    }
}
```

### Gradient Descent 


## Experiments

Now let's try using the implementation above to train a fully connected neural network for a classification task. For this we use two popular synthetic datasets. The first dataset consists of two noisy intertwined spirals. The second dataset follows the distribution of a checkerboard pattern. Both datasets used for training the model consisted of 20.000 samples. The validation datasets consisted both of 2000 points. The network consisted of three hidden layers with 64 neurons each. The model was trained for 1000 epochs at a learning rate of $$\eta=1e-4$$.

<p align="center"> 
<img src="/assets/images/post7/dataset_spirals.png" width="365">
<img src="/assets/images/post7/dataset_checkerboard2.png" width="365">
</p>

## Results

The following section shows the results for the validation dataset.

### Spirals

The results for the spirals show that after an initial phase of slow convergence of about 200 epochs, the model converges very quickly and finally reaches a validation accuracy of 99.2%.

<p align="center"> 
<img src="/assets/images/post7/loss_spirals.png" width="365">
<img src="/assets/images/post7/accuracy_spirals.png" width="365">
</p>

The following two figures show the network's corresponding continuous and discrete prediction landscape after 1000 epochs of training. Here we see that due to the low noise of both spirals, the network has learned to separate them fairly cleanly.

<p align="center"> 
<img src="/assets/images/post7/prediction_landscape_spirals.png" width="365">
<img src="/assets/images/post7/prediction_landscape_discrete_spirals.png" width="365">
</p>

### Checkerboard

For the checkerboard, the results are similarly good. The results show a relatively fast convergence of the model reaching 96.8 % validation accuracy after 1000 epochs.

<p align="center"> 
<img src="/assets/images/post7/loss_checkerboard.png" width="365">
<img src="/assets/images/post7/accuracy_checkerboard.png" width="365">
</p>

Below we see again the network's corresponding continuous and discrete prediction landscape for the checkerboard dataset. It is interesting to see how a simple three-layered model of 64 hidden neuron each, can separate the tiles of the checkerboard so cleanly.

<p align="center"> 
<img src="/assets/images/post7/prediction_landscape_checkerboard.png" width="365">
<img src="/assets/images/post7/prediction_landscape_discrete_checkerboard.png" width="365">
</p>

## Discussion

This project is a very basic implementation of a fully connected neural network in C++ with many possibilities for improvement. The results not only show good performance of the network on the selected data sets, but also indicate the correctness of the implementation.

## Outlook

The implementation shown in this post should serve as a gentle introduction on how to implement a simple neural networks in C++. The implementation of the fully connected neural network can be extended very easily to deal with an arbitrary number of hidden layers as well as different activation functions.

In addition, the code can be used to try different things. For example, certain layers can be frozen during training without much effort. Also, the implementation can be extended to handle batches. Last but not least, to speed up the computation you can try to parallelize the multiplication of matrices and vectors so that the program uses more than one core.

---

You find the code for this project [here][github_code].

<!-- Links -->

[github_code]: https://github.com/KaiFabi/MicroMLP
[he_paper]: https://arxiv.org/abs/1502.01852
